{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import argparse\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from herbie_vision.model.centernet import get_hourglass, exkp\n",
    "\n",
    "from herbie_vision.utils.image import annotations_to_df, process_resizing\n",
    "from herbie_vision.utils.gcp_utils import download_blob, upload_blob\n",
    "\n",
    "\n",
    "from google.api_core.protobuf_helpers import get_messages\n",
    "from google.cloud import storage\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/Users/peterfagan/Desktop/gcp/waymo-2d-object-detection-514eeefdb0a3.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_blob(bucket_name, source_blob_name, destination_file_name):\n",
    "    \"\"\"Downloads a blob from the bucket.\"\"\"\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(source_blob_name)\n",
    "    blob.download_to_filename(destination_file_name)\n",
    "    \n",
    "    print(\n",
    "        \"Blob {} downloaded to {}.\".format(\n",
    "            source_blob_name, destination_file_name\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    \n",
    "\n",
    "client = storage.Client()\n",
    "bucket = client.get_bucket('waymo-processed')\n",
    "\n",
    "download_blob('waymo-processed',\n",
    "              'train/images/2019-02-13/10017090168044687777_6380_000_6400_000/10017090168044687777_6380_000_6400_000_150_FRONT.jpeg',\n",
    "              '/Users/peterfagan/Code/Waymo-2D-Object-Detection/demo/example_1.jpeg')\n",
    "download_blob('waymo-processed',\n",
    "              'train/annotations/2019-02-13/10017090168044687777_6380_000_6400_000.json',\n",
    "              '/Users/peterfagan/Code/Waymo-2D-Object-Detection/demo/annotations.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in image annotations\n",
    "f = open('/Users/peterfagan/Code/Waymo-2D-Object-Detection/demo/annotations.json','r')\n",
    "annotations = json.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Bounding Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('/Users/peterfagan/Code/Waymo-2D-Object-Detection/demo/example_1.jpeg')\n",
    "rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "anns = [x for x in annotations['annotations'] if x['image_id']=='10017090168044687777_6380_000_6400_000_150_FRONT']\n",
    "\n",
    "# Create figure and axes\n",
    "fig, ax = plt.subplots(figsize = (200,20))\n",
    "\n",
    "# Display the image\n",
    "ax.imshow(rgb_img)\n",
    "\n",
    "for entry in anns:\n",
    "    h = entry['bbox'][3]\n",
    "    w = entry['bbox'][2]\n",
    "    x = entry['bbox'][0]\n",
    "    y = entry['bbox'][1]\n",
    "    \n",
    "    # Create a Rectangle patch\n",
    "    rect = patches.Rectangle((x,y), h, w, linewidth=2, edgecolor='r', facecolor='none')\n",
    "\n",
    "    # Add the patch to the Axes\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dataset locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_blob(bucket_name, source_blob_name, destination_file_name):\n",
    "    \"\"\"Downloads a blob from the bucket.\"\"\"\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(source_blob_name)\n",
    "    blob.download_to_filename(destination_file_name)\n",
    "\n",
    "    print(\n",
    "        \"Blob {} downloaded to {}.\".format(\n",
    "            source_blob_name, destination_file_name\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORY_NAMES = ['TYPE_VEHICLE','TYPE_PEDESTRIAN','TYPE_CYCLIST']\n",
    "CATEGORY_IDS = [1,2,4]\n",
    "\n",
    "class WaymoDataset(data.Dataset):\n",
    "    def __init__(self, gcp_bucket, gcp_annotations_path, local_path_to_images, \n",
    "                 local_path_to_processed_images, cat_names, cat_ids):\n",
    "        super(WaymoDataset, self).__init__()\n",
    "        \n",
    "        # filepaths\n",
    "        self.gcp_bucket = gcp_bucket\n",
    "        self.gcp_annotations_path = gcp_annotations_path\n",
    "        self.local_path_to_images = local_path_to_images\n",
    "        self.local_path_to_processed_images = local_path_to_processed_images\n",
    "        \n",
    "        # high level summary values\n",
    "        self.num_classes = len(cat_names)\n",
    "        self.category_names = cat_names\n",
    "        self.category_ids = cat_ids\n",
    "        \n",
    "        \n",
    "        # setup data directory\n",
    "        if os.path.exists('./data')==False:\n",
    "            os.mkdir('./data')\n",
    "            os.mkdir(self.local_path_to_images)\n",
    "            os.mkdir(self.local_path_to_processed_images)\n",
    "        \n",
    "        \n",
    "        # read in annotations\n",
    "        client = storage.Client()\n",
    "        bucket = client.get_bucket(self.gcp_bucket)\n",
    "        \n",
    "        download_blob(self.gcp_bucket,\n",
    "                           self.gcp_annotations_path,\n",
    "                           './data/annotations.json')\n",
    "        \n",
    "        f = open('./data/annotations.json','r')\n",
    "        self.annotations = json.load(f)\n",
    "        f.close()\n",
    "        \n",
    "        # convert annotations to dataframe\n",
    "        self.annotations_df = annotations_to_df(self.annotations, self.local_path_to_images)\n",
    "\n",
    "        \n",
    "        # determine segment paths\n",
    "        self.segment_paths = []\n",
    "        for image in self.annotations['images']:\n",
    "            uri = image['gcp_url']\n",
    "            segment = '/'.join(uri.split('/')[3:7])+'/'\n",
    "            if segment not in self.segment_paths:\n",
    "                self.segment_paths.append(segment)\n",
    "        \n",
    "        \n",
    "        # Download images for segments to local folder\n",
    "        for segment in self.segment_paths:\n",
    "            blobs = bucket.list_blobs(prefix=segment, delimiter='/')\n",
    "            for blob in list(blobs):\n",
    "                filename=blob.name.replace(segment,'')\n",
    "                blob.download_to_filename('./data/images/{}'.format(filename))\n",
    "\n",
    "\n",
    "        # Drop images without annotations\n",
    "        self.annotations['images'] = [x for x in self.annotations['images'] if x['id'] in self.annotations_df['image_id'].unique()]\n",
    "        \n",
    "        # Preprocess images to be the same size\n",
    "        self.annotations_df = process_resizing(self.local_path_to_processed_images, self.annotations_df,800)\n",
    "        \n",
    "            \n",
    "            \n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.annotations['images'][idx]['id']\n",
    "        image_url = self.annotations['images'][idx]['gcp_url']\n",
    "        filename = image_url.split('/')[-1]\n",
    "        image = Image.open(self.local_path_to_processed_images+'{}'.format(filename))\n",
    "        image = np.asarray(image, dtype=\"float64\") / 255.\n",
    "        image = torch.tensor(image).permute(2,0,1).float()        \n",
    "        \n",
    "        # define target data for fast rcnn\n",
    "        temp_df = self.annotations_df[self.annotations_df['image_id']==image_id]\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        areas = []\n",
    "        for _,item in temp_df.iterrows():\n",
    "            boxes.append([item['xr_min'],item['yr_min'],item['xr_max'],item['yr_max']])\n",
    "            labels.append(item['category_id'])\n",
    "            areas.append(item['area'])\n",
    "        \n",
    "        boxes = torch.tensor(boxes)\n",
    "        areas = torch.tensor(areas)\n",
    "        labels = torch.tensor(labels)\n",
    "        \n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = areas\n",
    "        target[\"iscrowd\"] = torch.zeros((temp_df.shape[0],), dtype=torch.int64)\n",
    "        \n",
    "        return image, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.annotations['images'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blob train/annotations/2019-05-22/11940460932056521663_1760_000_1780_000.json downloaded to ./data/annotations.json.\n"
     ]
    }
   ],
   "source": [
    "dataset = WaymoDataset('waymo-processed','train/annotations/2019-05-22/11940460932056521663_1760_000_1780_000.json','./data/images/','./data/images_processed/', CATEGORY_NAMES, CATEGORY_IDS)\n",
    "train_dataloader = data.DataLoader(dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resizing images and bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotations_to_df(annotations):\n",
    "    df = pd.DataFrame(annotations['annotations'])\n",
    "    df['filename'] = df['image_id'].apply(lambda x :os.getcwd()+'/data/images/{}.jpeg'.format(x))\n",
    "    df['x_min'] = df['bbox'].apply(lambda x: x[0])\n",
    "    df['y_min'] = df['bbox'].apply(lambda x: x[1])\n",
    "    df['width'] = df['bbox'].apply(lambda x: x[2])\n",
    "    df['height'] = df['bbox'].apply(lambda x: x[3])\n",
    "    df['x_max'] = df['x_min'] + df['height']\n",
    "    df['y_max'] = df['y_min'] + df['width']\n",
    "    df.drop(columns='bbox',inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def create_mask(bb, x):\n",
    "    \"\"\"Creates a mask for the bounding box of same shape as image\"\"\"\n",
    "    rows,cols,*_ = x.shape\n",
    "    Y = np.zeros((rows, cols))\n",
    "    bb = bb.astype(int)\n",
    "    Y[bb[1]:bb[3], bb[0]:bb[2]] = 1.\n",
    "    return Y\n",
    "\n",
    "def mask_to_bb(Y):\n",
    "    \"\"\"Convert mask Y to a bounding box, assumes 0 as background nonzero object\"\"\"\n",
    "    cols, rows = np.nonzero(Y)\n",
    "    if len(cols)==0: \n",
    "        return np.zeros(4, dtype=np.float32)\n",
    "    top_row = np.min(rows)\n",
    "    left_col = np.min(cols)\n",
    "    bottom_row = np.max(rows)\n",
    "    right_col = np.max(cols)\n",
    "    return np.array([top_row, left_col, bottom_row, right_col], dtype=np.float32)\n",
    "\n",
    "\n",
    "def resize_image_bb(read_path,write_path,bb,sz):\n",
    "    \"\"\"Resize an image and its bounding box and write image to new path\"\"\"\n",
    "    im = np.array(Image.open(read_path))\n",
    "    im_resized = cv2.resize(im, (sz, sz))\n",
    "    Y_resized = cv2.resize(create_mask(bb, im), (sz, sz))\n",
    "    new_path = write_path + read_path.split('/')[-1]\n",
    "    cv2.imwrite(new_path, cv2.cvtColor(im_resized, cv2.COLOR_RGB2BGR))\n",
    "    return new_path, mask_to_bb(Y_resized)\n",
    "\n",
    "\n",
    "def process_resizing(resized_path, annotations_df, sz):\n",
    "    new_paths = []\n",
    "    new_bbs = []\n",
    "    for index, row in annotations_df[['filename','x_min','y_min','x_max','y_max']].iterrows():\n",
    "        new_path,new_bb = resize_image_bb(row['filename'], resized_path,\n",
    "                                          np.array(row[['x_min','y_min','x_max','y_max']]),sz)\n",
    "        new_paths.append(new_path)\n",
    "        new_bbs.append(new_bb)\n",
    "    annotations_df['processed_filepath'] = new_paths\n",
    "    annotations_df['resized_bb'] = new_bbs\n",
    "    annotations_df['xr_min'] = annotations_df['resized_bb'].apply(lambda x: x[0])\n",
    "    annotations_df['yr_min'] = annotations_df['resized_bb'].apply(lambda x: x[1])\n",
    "    annotations_df['xr_max'] = annotations_df['resized_bb'].apply(lambda x: x[2])\n",
    "    annotations_df['yr_max'] = annotations_df['resized_bb'].apply(lambda x: x[3])\n",
    "    annotations_df.drop(columns='resized_bb',inplace=True)\n",
    "    \n",
    "    \n",
    "    return annotations_df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_df = annotations_to_df(dataset.annotations)\n",
    "process_resizing('./data/images_processed/',annotations_df, 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resized_img = cv2.resize(orig_img,(800,800))\n",
    "# bb = np.array([example['x_min'][0],example['y_min'][0],\n",
    "#               example['x_max'][0],example['y_max'][0]])\n",
    "# mask = create_mask(bb,orig_img)\n",
    "# resized_mask = cv2.resize(mask,(800,800))\n",
    "# resized_bb = mask_to_bb(resized_mask)\n",
    "\n",
    "# rect = patches.Rectangle((bb[0],bb[1]), h, w, linewidth=2, edgecolor='r', facecolor='none')\n",
    "\n",
    "\n",
    "# # Create figure and axes\n",
    "# fig, ax = plt.subplots(figsize = (200,20))\n",
    "\n",
    "# # Display the image\n",
    "# ax.imshow(resized_img)\n",
    "\n",
    "# # Create a Rectangle patch\n",
    "# rect = patches.Rectangle((resized_bb[1],resized_bb[0]), resized_bb[2]-resized_bb[0], resized_bb[3]-resized_bb[1], linewidth=2, edgecolor='r', facecolor='none')\n",
    "\n",
    "# # Add the patch to the Axes\n",
    "# ax.add_patch(rect)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_df = annotations_to_df(dataset.annotations)\n",
    "example = annotations_df[annotations_df['image_id']=='10017090168044687777_6380_000_6400_000_150_FRONT'].reset_index()\n",
    "orig_img = np.array(Image.open(example['filename'].unique()[0]))\n",
    "\n",
    "def plot_w_annotations(img, annotations_df, resized=False):    \n",
    "    # Create figure and axes\n",
    "    fig, ax = plt.subplots(figsize = (200,20))\n",
    "    \n",
    "    # Display the image\n",
    "    ax.imshow(img)\n",
    "\n",
    "    for entry in annotations_df.iterrows():\n",
    "        if resized==False:\n",
    "            h = entry[1]['height']\n",
    "            w = entry[1]['width']\n",
    "            x = entry[1]['x_min']\n",
    "            y = entry[1]['y_min']\n",
    "        else:\n",
    "            h = entry[1]['xr_max']-entry[1]['xr_min']\n",
    "            w = entry[1]['yr_max']-entry[1]['yr_min']\n",
    "            x = entry[1]['xr_min']\n",
    "            y = entry[1]['yr_min']\n",
    "\n",
    "        # Create a Rectangle patch\n",
    "        rect = patches.Rectangle((x,y), h, w, linewidth=2, edgecolor='r', facecolor='none')\n",
    "\n",
    "        # Add the patch to the Axes\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "plot_w_annotations(orig_img, example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df = process_resizing('./data/images_processed/', example, 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resized_img = np.array(Image.open(example['processed_filepath'].unique()[0]))\n",
    "plot_w_annotations(resized_img, updated_df, resized=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(resized_img).permute(2,0,1).unsqueeze(0)/255.\n",
    "outputs = model(inputs)\n",
    "len(outputs[0]['boxes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resized_img = cv2.resize(orig_img,(800,800))\n",
    "bb = np.array([example['x_min'][0],example['y_min'][0],\n",
    "              example['x_max'][0],example['y_max'][0]])\n",
    "mask = create_mask(bb,orig_img)\n",
    "resized_mask = cv2.resize(mask,(800,800))\n",
    "resized_bb = mask_to_bb(resized_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refining Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "import os\n",
    "from herbie_vision.utils.image import annotations_to_df, process_resizing\n",
    "from herbie_vision.utils.gcp_utils import download_blob, upload_blob\n",
    "\n",
    "CATEGORY_NAMES = ['TYPE_VEHICLE','TYPE_PEDESTRIAN','TYPE_CYCLIST']\n",
    "CATEGORY_IDS = [1,2,4]\n",
    "\n",
    "class WaymoDataset(data.Dataset):\n",
    "    def __init__(self, gcp_bucket, gcp_annotations_path, local_path_to_images, \n",
    "                 local_path_to_processed_images, cat_names, cat_ids):\n",
    "        super(WaymoDataset, self).__init__()\n",
    "        \n",
    "        # filepaths\n",
    "        self.gcp_bucket = gcp_bucket\n",
    "        self.gcp_annotations_path = gcp_annotations_path\n",
    "        self.local_path_to_images = local_path_to_images\n",
    "        self.local_path_to_processed_images = local_path_to_processed_images\n",
    "        \n",
    "        # high level summary values\n",
    "        self.num_classes = len(cat_names)\n",
    "        self.category_names = cat_names\n",
    "        self.category_ids = cat_ids\n",
    "        \n",
    "        \n",
    "        # setup data directory\n",
    "        if os.path.exists('./data')==False:\n",
    "            os.mkdir('./data')\n",
    "            os.mkdir(self.path_to_images)\n",
    "            os.mkdir(self.path_to_processed_images)\n",
    "        \n",
    "        \n",
    "        # read in annotations\n",
    "        client = storage.Client()\n",
    "        bucket = client.get_bucket(self.gcp_bucket)\n",
    "        \n",
    "        download_blob(self.gcp_bucket,\n",
    "                           self.gcp_annotations_path,\n",
    "                           './data/annotations.json')\n",
    "        \n",
    "        f = open('./data/annotations.json','r')\n",
    "        self.annotations = json.load(f)\n",
    "        f.close()\n",
    "        \n",
    "        # convert annotations to dataframe\n",
    "        self.annotations_df = annotations_to_df(self.annotations)\n",
    "\n",
    "        \n",
    "        # determine segment paths\n",
    "#         self.segment_paths = []\n",
    "#         for image in self.annotations['images']:\n",
    "#             uri = image['gcp_url']\n",
    "#             segment = '/'.join(uri.split('/')[3:7])+'/'\n",
    "#             if segment not in self.segment_paths:\n",
    "#                 self.segment_paths.append(segment)\n",
    "        \n",
    "        \n",
    "        # Download images for segments to local folder\n",
    "#         for segment in self.segment_paths:\n",
    "#             blobs = bucket.list_blobs(prefix=segment, delimiter='/')\n",
    "#             for blob in list(blobs):\n",
    "#                 filename=blob.name.replace(segment,'')\n",
    "#                 blob.download_to_filename('./data/images/{}'.format(filename))\n",
    "\n",
    "\n",
    "#         # Drop images without annotations\n",
    "        self.annotations['images'] = [x for x in self.annotations['images'] if x['id'] in self.annotations_df['image_id'].unique()]\n",
    "        \n",
    "#         # Preprocess images to be the same size\n",
    "#         self.annotations_df = process_resizing(self.local_path_to_processed_images, self.annotations_df,800)\n",
    "        \n",
    "#         self.annotations_df.to_csv('./data/annotations.csv')\n",
    "\n",
    "        self.annotations_df = pd.read_csv('./data/annotations.csv')\n",
    "            \n",
    "            \n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.annotations['images'][idx]['id']\n",
    "        image_url = self.annotations['images'][idx]['gcp_url']\n",
    "        filename = image_url.split('/')[-1]\n",
    "        image = Image.open(self.local_path_to_processed_images+'{}'.format(filename))\n",
    "        image = np.asarray(image, dtype=\"float64\") / 255.\n",
    "        image = torch.tensor(image).permute(2,0,1).float()        \n",
    "        \n",
    "        # define target data for fast rcnn\n",
    "        temp_df = self.annotations_df[self.annotations_df['image_id']==image_id]\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        areas = []\n",
    "        for _,item in temp_df.iterrows():\n",
    "            boxes.append([item['xr_min'],item['yr_min'],item['xr_max'],item['yr_max']])\n",
    "            labels.append(item['category_id'])\n",
    "            areas.append(item['area'])\n",
    "        \n",
    "        boxes = torch.tensor(boxes)\n",
    "        areas = torch.tensor(areas)\n",
    "        labels = torch.tensor(labels)\n",
    "        \n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = areas\n",
    "        target[\"iscrowd\"] = torch.zeros((temp_df.shape[0],), dtype=torch.int64)\n",
    "        \n",
    "        return image, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.annotations['images'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "dataset = WaymoDataset('waymo-processed','train/annotations/2019-02-13/10017090168044687777_6380_000_6400_000.json','./data/images/','./data/images_processed/', CATEGORY_NAMES, CATEGORY_IDS)\n",
    "test = data.Subset(dataset, indices=[1,2])\n",
    "train_dataloader = data.DataLoader(test, batch_size=4, \n",
    "                                   shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Playground"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faster rcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "# Initialize model and set to eval mode\n",
    "def get_model_instance_segmentation(num_classes):\n",
    "    # load an instance segmentation model pre-trained pre-trained on COCO\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False)\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model\n",
    "\n",
    "# 2 classes; Only target class or background\n",
    "num_classes = 3\n",
    "num_epochs = 25\n",
    "model = get_model_instance_segmentation(num_classes)\n",
    "\n",
    "# move model to the right device\n",
    "    \n",
    "# parameters\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "len_dataloader = len(train_dataloader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tens = torch.randn(3,800,800).unsqueeze(0)\n",
    "model.eval()\n",
    "model(tens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    i = 0    \n",
    "    for imgs, annotations in train_dataloader:\n",
    "        i += 1\n",
    "        imgs = list(img for img in imgs)\n",
    "        annotations = [{k: v for k, v in t.items()} for t in annotations]\n",
    "        loss_dict = model(imgs, annotations)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f'Iteration: {i}/{len_dataloader}, Loss: {losses}')\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for img, _ in train_dataloader:\n",
    "    outputs = model(img)\n",
    "    img = np.array(img[0].permute(1,2,0))\n",
    "    plt.imshow(img)\n",
    "    print(outputs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in outputs[0]['boxes']:\n",
    "    print(entry.detach().numpy())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in outputs[0]['boxes'][0]:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_outputs(img, bbox):    \n",
    "    # Create figure and axes\n",
    "    fig, ax = plt.subplots(figsize = (200,20))\n",
    "    \n",
    "    # Display the image\n",
    "    ax.imshow(img)\n",
    "    i=0\n",
    "    for entry in bbox:\n",
    "        \n",
    "        entry=entry.detach().numpy()\n",
    "        h = entry[2]-entry[0]\n",
    "        w = entry[3]-entry[1]\n",
    "        # Create a Rectangle patch\n",
    "        rect = patches.Rectangle((entry[0],entry[1]), h, w, linewidth=2, edgecolor='r', facecolor='none')\n",
    "\n",
    "        # Add the patch to the Axes\n",
    "        ax.add_patch(rect)\n",
    "        i+=1\n",
    "        if i==10:\n",
    "            break\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "plot_outputs(img, outputs[0]['boxes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centernet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model\n",
    "model = exkp(n=5, nstack=2, dims=[256, 256, 384, 384, 384, 512], modules=[2, 2, 2, 2, 2, 4],num_classes=3)\n",
    "model.train()\n",
    "\n",
    "    \n",
    "inputs = torch.randn(1,3,512,512)\n",
    "outputs = model(inputs)\n",
    "hmap_tl, hmap_br, hmap_ct, embd_tl, embd_br, regs_tl, regs_br, regs_ct = zip(*outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
